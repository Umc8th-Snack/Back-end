package umc.snack.crawler;

import org.junit.jupiter.api.Tag;
import org.junit.jupiter.api.Test;
import org.mockito.ArgumentCaptor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.test.context.bean.override.mockito.MockitoSpyBean;
import umc.snack.crawler.service.ArticleCollectorService;
import umc.snack.crawler.service.ArticleCrawlerService;
import umc.snack.domain.article.entity.CrawledArticle;
import umc.snack.repository.article.CrawledArticleRepository;

import java.nio.file.Files;
import java.nio.file.Path;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.stream.Collectors;

import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.mockito.Mockito.atLeast;
import static org.mockito.Mockito.verify;

@SpringBootTest
@Tag("live")
public class CrawlQualityTest {

    @Autowired
    private ArticleCrawlerService articleCrawlerService;

    @Autowired
    private ArticleCollectorService articleCollectorService;

    @MockitoSpyBean
    private CrawledArticleRepository crawledArticleRepository;

    @Test
    void crawlDataQualityShouldBeWithinThreshold() throws Exception {
        int sampleSize = Integer.parseInt(System.getProperty("crawl.sample", "500"));
        double maxNullPercent = Double.parseDouble(System.getProperty("crawl.maxNullPercent", "20"));

        // 1) DBì—ì„œ ìµœì‹  ê¸°ì‚¬ URL ìˆ˜ì§‘ (ì¹´í…Œê³ ë¦¬ë³„ 10ê°œ, IT/ê³¼í•™ ê°€ì¤‘ 2, ì–¸ë¡ ì‚¬ ìƒí•œ 3)
        java.util.Map<String, Integer> weights = new java.util.HashMap<>();
        weights.put("100", 1);
        weights.put("101", 1);
        weights.put("102", 1);
        weights.put("103", 1);
        weights.put("104", 1);
        weights.put("105", 2);
        int perPublisherLimit = 3;

        List<String> links = articleCollectorService.collectArticleLinksPerCategoryWeighted(10, weights, perPublisherLimit)
                .stream()
                .limit(sampleSize)
                .toList();

        if (links.isEmpty()) {
            throw new IllegalStateException("DB/ìˆ˜ì§‘ê¸°ì—ì„œ ê°€ì ¸ì˜¨ ì‹ ê·œ ê¸°ì‚¬ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ì§‘ ë¡œì§ í˜¹ì€ ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.");
        }

        // 2) JSON ìƒì„± í›„ ì‹¤ì œ í¬ë¡¤ ì‹¤í–‰
        String json = articleCollectorService.toJson(links);

        articleCrawlerService.crawlFromJson(json);

        // 3) ì´ë²ˆ í…ŒìŠ¤íŠ¸ì—ì„œ ì €ì¥ëœ ì—”í‹°í‹° ìº¡ì²˜í•˜ì—¬ í’ˆì§ˆ ê³„ì‚°
        ArgumentCaptor<CrawledArticle> captor = ArgumentCaptor.forClass(CrawledArticle.class);
        verify(crawledArticleRepository, atLeast(1)).save(captor.capture());

        Set<String> target = new HashSet<>(links);

        List<CrawledArticle> processed = captor.getAllValues().stream()
                .filter(ca -> target.contains(ca.getArticleUrl()))
                .filter(ca -> ca.getStatus() == CrawledArticle.Status.PROCESSED)
                .toList();

        long total = processed.size();
        if (total == 0) {
            throw new AssertionError("âŒ PROCESSED ìƒíƒœë¡œ ì €ì¥ëœ ì•„í‹°í´ì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.");
        }

        long nullCount = processed.stream()
                .filter(ca -> ca.getContent() == null || ca.getContent().isBlank())
                .count();

        double nullPercent = (double) nullCount / total * 100.0;

        System.out.printf("ğŸ“Š ë¼ì´ë¸Œ ì´ ê°œìˆ˜: %d | ë¹ˆ ê°’ ê°œìˆ˜: %d | ë¹ˆ ê°’ ë¹„ìœ¨: %.2f%%%n",
                total, nullCount, nullPercent);

        assertTrue(
                nullPercent <= maxNullPercent,
                String.format("âŒ ë¼ì´ë¸Œ ë¹ˆ ê°’ ë¹„ìœ¨ %.2f%% ì´(ê°€) í—ˆìš© ê¸°ì¤€ %.2f%% ì„(ë¥¼) ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.", nullPercent, maxNullPercent)
        );

        System.out.println("âœ… ë¼ì´ë¸Œ í¬ë¡¤ í’ˆì§ˆì´ í—ˆìš© ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤.");
    }
}