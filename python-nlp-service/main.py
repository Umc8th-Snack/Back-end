# main.py
from fastapi import FastAPI, HTTPException, status, Query
from pydantic import BaseModel
import uvicorn
import asyncio
import logging
from typing import List, Dict, Any, Optional
import aiomysql
import os
from dotenv import load_dotenv
import numpy as np
import json

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ìˆì—ˆìŒ!) ---
app = FastAPI(
    title="ê¸°ì‚¬ NLP ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤",
    description="ê¸°ì‚¬ ë¶„ì„ ë° ì¶”ì²œì„ ìœ„í•œ í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ (TF-IDF + SBERT)",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# DB ì—°ê²° ì„¤ì • (.envì—ì„œ ì½ê¸°)
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': int(os.getenv('DB_PORT', 3306)),
    'user': os.getenv('DB_USER', 'root'),
    'password': os.getenv('DB_PASSWORD', ''),
    'db': os.getenv('DB_NAME', 'snack_db'),
    'charset': os.getenv('DB_CHARSET', 'utf8mb4'),
    'autocommit': True
}

# DB ì—°ê²° í’€ (ì „ì—­ ë³€ìˆ˜)
db_pool = None

# NLP í”„ë¡œì„¸ì„œ import (ì—†ìœ¼ë©´ ì„ì‹œ ëª¨ë“ˆ)
try:
    import nlp_processor
except ImportError:
    logger.warning("nlp_processor ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê¸°ëŠ¥ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    nlp_processor = None

# --- Pydantic ëª¨ë¸ ì •ì˜ ---

class ArticleVectorizeRequestDto(BaseModel):
    articleId: int
    title: str
    summary: str

class ArticleVectorizeListRequestDto(BaseModel):
    articles: List[ArticleVectorizeRequestDto]

class ArticleKeywordDto(BaseModel):
    word: str
    tfidf: float

class ArticleVectorizeResponseDto(BaseModel):
    articleId: int
    vector: List[float]
    keywords: List[ArticleKeywordDto]

class ArticleVectorizeListResponseDto(BaseModel):
    results: List[ArticleVectorizeResponseDto]

class ArticleDto(BaseModel):
    articleId: int
    title: str
    summary: str

class SearchArticleResponseDto(BaseModel):
    articles: List[ArticleDto]

# DTO ëª¨ë¸ ì •ì˜
class KeywordScore(BaseModel):
    word: str
    tfidf: float

class ArticleSearchResult(BaseModel):
    article_id: int
    title: str
    summary: Optional[str] = None
    score: float
    # keywords: List[KeywordScore]
    publishedAt: Optional[str]

class SearchResponse(BaseModel):
    query: str
    totalCount: int
    articles: List[ArticleSearchResult]

class UserInteraction(BaseModel):
    articleId: int
    action: str # "click" or "scrap"

class UserProfileRequest(BaseModel):
    userId: int
    interactions: List[UserInteraction]

class RecommendedArticle(BaseModel):
    articleId: int
    score: float

class FeedResponse(BaseModel):
    articles: List[RecommendedArticle]

# --- ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---

@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì‹¤í–‰"""
    global db_pool
    logger.info("=" * 50)
    logger.info("NLP ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘...")
    logger.info(f"DB ì—°ê²° ì„¤ì •: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['db']}")

    try:
        # DB ì—°ê²° í’€ ìƒì„±
        db_pool = await aiomysql.create_pool(
            **DB_CONFIG,
            minsize=5,
            maxsize=20
        )
        logger.info("âœ… DB ì—°ê²° í’€ ìƒì„± ì„±ê³µ")

        # ì—°ê²° í…ŒìŠ¤íŠ¸
        async with db_pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute("SELECT 1")
                result = await cursor.fetchone()
                logger.info(f"âœ… DB ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ: {result}")

        # NLP ëª¨ë¸ ì´ˆê¸°í™”
        if nlp_processor:
            await nlp_processor.initialize_nlp_service()
            logger.info("âœ… NLP ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        else:
            logger.warning("âš ï¸ NLP ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        logger.info("=" * 50)
        logger.info("ğŸš€ ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")

    except Exception as e:
        logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.error("ì„œë²„ëŠ” ê³„ì† ì‹¤í–‰ë˜ì§€ë§Œ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

@app.on_event("shutdown")
async def shutdown_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ì‹¤í–‰"""
    global db_pool
    logger.info("ì„œë¹„ìŠ¤ ì¢…ë£Œ ì¤‘...")

    if db_pool:
        db_pool.close()
        await db_pool.wait_closed()
        logger.info("DB ì—°ê²° í’€ ì¢…ë£Œ ì™„ë£Œ")

# --- í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ---

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "NLP Microservice",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/health")
async def health_check():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    status = {
        "service": "healthy",
        "database": "unknown",
        "nlp_model": "unknown"
    }

    # DB ì—°ê²° í™•ì¸
    if db_pool:
        try:
            async with db_pool.acquire() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute("SELECT 1")
                    status["database"] = "connected"
        except:
            status["database"] = "disconnected"

    # NLP ëª¨ë¸ í™•ì¸
    if nlp_processor and hasattr(nlp_processor, 'is_service_ready'):
        status["nlp_model"] = "loaded" if nlp_processor.is_service_ready() else "not loaded"

    return status

# --- ë²¡í„°í™” ì—”ë“œí¬ì¸íŠ¸ ---
# main.pyì˜ ìˆ˜ì •ëœ ë²¡í„°í™” í•¨ìˆ˜ ë¶€ë¶„

@app.post("/api/nlp/vectorize/articles")
async def vectorize_articles_from_db(article_ids: List[int]):
    if not article_ids:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="article_idsëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
    if not db_pool:
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")

    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            processed_count, failed_ids = 0, []
            for article_id in article_ids:
                try:
                    await cursor.execute("SELECT article_id, title, summary FROM articles WHERE article_id = %s", (article_id,))
                    article = await cursor.fetchone()
                    if not article:
                        logger.warning(f"ê¸°ì‚¬ ID {article_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        failed_ids.append(article_id)
                        continue

                    text = article.get('summary') or ''

                    tfidf_keywords = {}
                    sbert_vectors = {}
                    representative_vector = [0.0] * 768

                    if not text.strip():
                        # summaryê°€ ë¹„ì–´ìˆìœ¼ë©´, ë²¡í„°ì™€ í‚¤ì›Œë“œë¥¼ ëª¨ë‘ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì²˜ë¦¬
                        logger.info(f"ê¸°ì‚¬ {article_id}ëŠ” summaryê°€ ì—†ì–´ ë¹ˆ ê°’ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                    elif nlp_processor:
                        # summaryê°€ ìˆì„ ë•Œë§Œ NLP ì²˜ë¦¬
                        tfidf_keywords = await nlp_processor.extract_tfidf_keywords(text, top_k=10)
                        top_keywords = list(tfidf_keywords.keys())
                        if top_keywords:
                            sbert_vectors = await nlp_processor.generate_sbert_vectors(top_keywords)

                            # ëŒ€í‘œ ë²¡í„° ê³„ì‚° (í‚¤ì›Œë“œ ë²¡í„°ë“¤ì˜ ë‹¨ìˆœ í‰ê· )
                            if sbert_vectors:
                                all_vectors = np.array(list(sbert_vectors.values()))
                                avg_vector = np.mean(all_vectors, axis=0)
                                norm = np.linalg.norm(avg_vector)
                                if norm > 0:
                                    representative_vector = (avg_vector / norm).tolist()

                    vector_json_str = json.dumps(sbert_vectors)
                    keywords_json_str = json.dumps(tfidf_keywords)
                    rep_vector_str = json.dumps(representative_vector)

                    await cursor.execute("SELECT article_id FROM article_semantic_vectors WHERE article_id = %s", (article_id,))
                    existing = await cursor.fetchone()
                    if existing:
                        await cursor.execute(
                            "UPDATE article_semantic_vectors SET vector = %s, keywords = %s, representative_vector = %s, model_version = %s, updated_at = NOW() WHERE article_id = %s",
                            (vector_json_str, keywords_json_str, rep_vector_str, "sbert-keywords-v4", article_id)
                        )
                        logger.info(f"ê¸°ì‚¬ {article_id} í‚¤ì›Œë“œ ë° ëŒ€í‘œ ë²¡í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                    else:
                        await cursor.execute(
                            "INSERT INTO article_semantic_vectors (article_id, vector, keywords, representative_vector, model_version, created_at, updated_at) VALUES (%s, %s, %s, %s, %s, NOW(), NOW())",
                            (article_id, vector_json_str, keywords_json_str, rep_vector_str, "sbert-keywords-v4")
                        )
                        logger.info(f"ê¸°ì‚¬ {article_id} í‚¤ì›Œë“œ ë° ëŒ€í‘œ ë²¡í„° ì‹ ê·œ ì €ì¥ ì™„ë£Œ")

                    processed_count += 1

                except Exception as e:
                    logger.error(f"ê¸°ì‚¬ {article_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                    failed_ids.append(article_id)
                    continue
        return {"status": "completed", "total_requested": len(article_ids), "processed": processed_count, "failed": failed_ids}

@app.post("/api/nlp/vectorize/batch")
async def batch_vectorize_articles(
        limit: int = Query(100, description="í•œ ë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜"),
        force_update: bool = Query(False, description="ê¸°ì¡´ ë²¡í„° ì¬ìƒì„± ì—¬ë¶€")
):
    """
    ë²¡í„°í™”ë˜ì§€ ì•Šì€ ê¸°ì‚¬ë“¤ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """

    if not db_pool:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            # ë²¡í„°í™”ë˜ì§€ ì•Šì€ ê¸°ì‚¬ ì°¾ê¸°
            if force_update:
                # ê°•ì œ ì—…ë°ì´íŠ¸: ëª¨ë“  ê¸°ì‚¬ ëŒ€ìƒ
                query = """
                    SELECT article_id 
                    FROM articles 
                    ORDER BY created_at DESC 
                    LIMIT %s
                """
            else:
                # ë²¡í„°ê°€ ì—†ëŠ” ê¸°ì‚¬ë§Œ ì„ íƒ
                query = """
                    SELECT a.article_id 
                    FROM articles a
                    LEFT JOIN article_semantic_vectors asv 
                        ON a.article_id = asv.article_id
                    WHERE asv.article_id IS NULL
                    ORDER BY a.created_at DESC
                    LIMIT %s
                """

            await cursor.execute(query, (limit,))
            articles = await cursor.fetchall()

            if not articles:
                return {
                    "status": "no_articles",
                    "message": "ì²˜ë¦¬í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            article_ids = [a['article_id'] for a in articles]
            logger.info(f"ë²¡í„°í™”í•  ê¸°ì‚¬ {len(article_ids)}ê°œ ë°œê²¬: {article_ids[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸

            # ë²¡í„°í™” ìˆ˜í–‰
            return await vectorize_articles_from_db(article_ids)

# í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/db/check-schema")
async def check_db_schema():
    """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì™€ ë°ì´í„° ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""

    if not db_pool:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            result = {}

            try:
                # article_semantic_vectors í…Œì´ë¸” êµ¬ì¡° í™•ì¸
                await cursor.execute("""
                    SHOW COLUMNS FROM article_semantic_vectors
                """)
                result['article_semantic_vectors_columns'] = await cursor.fetchall()
            except Exception as e:
                result['article_semantic_vectors_error'] = str(e)

            try:
                # articles í…Œì´ë¸” êµ¬ì¡° í™•ì¸
                await cursor.execute("""
                    SHOW COLUMNS FROM articles
                """)
                result['articles_columns'] = await cursor.fetchall()
            except Exception as e:
                result['articles_error'] = str(e)

            try:
                # í…Œì´ë¸” ë°ì´í„° ê°œìˆ˜ í™•ì¸
                await cursor.execute("""
                    SELECT 
                        (SELECT COUNT(*) FROM articles) as article_count,
                        (SELECT COUNT(*) FROM article_semantic_vectors) as vector_count
                """)
                counts = await cursor.fetchone()
                result['data_counts'] = counts

                # ë²¡í„°í™”ë˜ì§€ ì•Šì€ ê¸°ì‚¬ ê°œìˆ˜
                await cursor.execute("""
                    SELECT COUNT(*) as unvectorized_count
                    FROM articles a
                    LEFT JOIN article_semantic_vectors asv 
                        ON a.article_id = asv.article_id
                    WHERE asv.article_id IS NULL
                """)
                unvectorized = await cursor.fetchone()
                result['unvectorized_articles'] = unvectorized['unvectorized_count']

            except Exception as e:
                result['count_error'] = str(e)

            return result

# í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸ - íŠ¹ì • ê¸°ì‚¬ì˜ ë²¡í„° í™•ì¸
@app.get("/api/vectors/{article_id}")
async def get_article_vector(article_id: int):
    """íŠ¹ì • ê¸°ì‚¬ì˜ ë²¡í„° ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""

    if not db_pool:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            # ê¸°ì‚¬ ì •ë³´ ì¡°íšŒ
            await cursor.execute("""
                SELECT a.article_id, a.title, a.summary,
                       asv.vector, asv.keywords, asv.model_version,
                       asv.created_at, asv.updated_at
                FROM articles a
                LEFT JOIN article_semantic_vectors asv 
                    ON a.article_id = asv.article_id
                WHERE a.article_id = %s
            """, (article_id,))

            result = await cursor.fetchone()

            if not result:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"ê¸°ì‚¬ ID {article_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )

            # ë²¡í„°ê°€ ìˆìœ¼ë©´ ê¸¸ì´ ê³„ì‚°
            if result['vector']:
                vector_str = result['vector'].strip('[]')
                vector_length = len(vector_str.split(',')) if vector_str else 0
                result['vector_length'] = vector_length
                # ë²¡í„° ë‚´ìš©ì€ ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                vector_preview = ','.join(vector_str.split(',')[:10])
                result['vector_preview'] = f"[{vector_preview},...]"
            else:
                result['vector_length'] = 0
                result['vector_preview'] = None

            return result

# --- ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸ ---
'''
@app.get("/api/articles/search/{keyword}")
async def search_articles_direct(
        keyword: str,
        page: int = Query(0, ge=0),
        size: int = Query(10, ge=1, le=50),
        threshold: float = Query(0.3, description="ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’")
):
    """í‚¤ì›Œë“œë¥¼ ë²¡í„°í™”í•˜ê³  DBì˜ ë²¡í„°ë“¤ê³¼ ì§ì ‘ ë¹„êµí•˜ì—¬ ìœ ì‚¬í•œ ê¸°ì‚¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""

    if not keyword or not keyword.strip():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="keywordëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤."
        )

    # ê°„ë‹¨í•œ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” nlp_processor ì‚¬ìš©)
    return SearchArticleResponseDto(articles=[])
'''
@app.get("/api/articles/search")
async def search_articles_semantic(
        query: str = Query(..., description="ê²€ìƒ‰í•  ë‹¨ì–´"),
        page: int = Query(0, ge=0, description="í˜ì´ì§€ ë²ˆí˜¸"),
        size: int = Query(10, ge=1, le=50, description="í˜ì´ì§€ í¬ê¸°"),
        threshold: float = Query(0.3, ge=0, le=1, description="ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’")
):
    """
    ì˜ë¯¸ ê¸°ë°˜ ê¸°ì‚¬ ê²€ìƒ‰ (í‚¤ì›Œë“œ ë²¡í„° ì§ì ‘ ë¹„êµ ë°©ì‹)
    """
    if not db_pool:
        raise HTTPException(status_code=503, detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")

    logger.info(f"ğŸ” ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹œì‘ - ê²€ìƒ‰ì–´: '{query}'")
    query_vector = await vectorize_raw_query(query)
    if query_vector is None:
        return SearchResponse(query=query, totalCount=0, articles=[])

    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            await cursor.execute(
                "SELECT a.article_id, a.title, a.summary, a.published_at, asv.vector, asv.keywords "
                "FROM articles a INNER JOIN article_semantic_vectors asv ON a.article_id = asv.article_id "
                "WHERE asv.vector IS NOT NULL AND JSON_LENGTH(asv.vector) > 0"
            )
            all_articles = await cursor.fetchall()
            if not all_articles:
                return SearchResponse(query=query, totalCount=0, articles=[])

            search_results = []
            for article in all_articles:
                try:
                    keyword_vectors = json.loads(article['vector'])
                    similarities = [cosine_similarity(query_vector, np.array(vec)) for vec in keyword_vectors.values()]
                    max_similarity = max(similarities) if similarities else 0.0

                    if max_similarity >= threshold:
                        search_results.append({
                            'article_id': article['article_id'],
                            'title': article['title'],
                            'summary': article['summary'],
                            'score': float(max_similarity),
                            'publishedAt': article['published_at'].isoformat() if article['published_at'] else None
                        })
                except Exception:
                    continue

            search_results.sort(key=lambda x: x['score'], reverse=True)
            paginated_results = search_results[page * size : (page + 1) * size]

            # í‚¤ì›Œë“œëŠ” ë°˜í™˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ArticleSearchResult ìƒì„± ì‹œ keywords ì œì™¸
            articles_response = [ArticleSearchResult(**{k: v for k, v in res.items() if k != 'keywords'}) for res in paginated_results]

            return SearchResponse(query=query, totalCount=len(search_results), articles=articles_response)

# --- í—¬í¼ í•¨ìˆ˜ ---

async def calculate_weighted_average(
        sbert_vectors: dict,
        tfidf_scores: dict
) -> List[float]:
    """TF-IDF ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•˜ì—¬ SBERT ë²¡í„°ë“¤ì˜ ê°€ì¤‘ í‰ê·  ê³„ì‚°"""

    if not sbert_vectors:
        return [0.0] * 384

    vectors = []
    weights = []

    for keyword in sbert_vectors:
        if keyword in tfidf_scores:
            vectors.append(sbert_vectors[keyword])
            weights.append(tfidf_scores[keyword])

    if not vectors:
        return [0.0] * 384

    # numpy ë°°ì—´ë¡œ ë³€í™˜
    vectors = np.array(vectors)
    weights = np.array(weights)

    # ê°€ì¤‘ì¹˜ ì •ê·œí™”
    weights = weights / weights.sum()

    # ê°€ì¤‘ í‰ê·  ê³„ì‚°
    weighted_avg = np.average(vectors, axis=0, weights=weights)

    # ë²¡í„° ì •ê·œí™”
    norm = np.linalg.norm(weighted_avg)
    if norm > 0:
        weighted_avg = weighted_avg / norm

    return weighted_avg.tolist()

@app.post("/api/nlp/search")
async def search_articles_semantic(
        query: str,
        page: int = Query(0, ge=0, description="í˜ì´ì§€ ë²ˆí˜¸"),
        size: int = Query(10, ge=1, le=50, description="í˜ì´ì§€ í¬ê¸°"),
        threshold: float = Query(0.3, ge=0, le=1, description="ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’")
):
    """
    ì˜ë¯¸ ê¸°ë°˜ ê¸°ì‚¬ ê²€ìƒ‰
    1. ê²€ìƒ‰ì–´ë¥¼ TF-IDFë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
    2. í‚¤ì›Œë“œë“¤ì˜ SBERT ë²¡í„° ìƒì„±
    3. TF-IDF ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì¿¼ë¦¬ ë²¡í„° ìƒì„±
    4. DBì˜ ëª¨ë“  ê¸°ì‚¬ ë²¡í„°ì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    5. ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
    """

    if not query or not query.strip():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
        )

    if not db_pool:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    logger.info(f"ğŸ” ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹œì‘ - ê²€ìƒ‰ì–´: '{query}', í˜ì´ì§€: {page}, í¬ê¸°: {size}")

    try:
        # 1. ê²€ìƒ‰ì–´ ë²¡í„°í™”
        query_vector = await vectorize_raw_query(query)

        if query_vector is None:
            logger.error("ê²€ìƒ‰ì–´ ë²¡í„°í™” ì‹¤íŒ¨")
            return SearchResponse(query=query, totalCount=0, articles=[])

        # 2. DBì—ì„œ ëª¨ë“  ê¸°ì‚¬ì™€ ë²¡í„° ì¡°íšŒ
        async with db_pool.acquire() as conn:
            async with conn.cursor(aiomysql.DictCursor) as cursor:
                # ëª¨ë“  ê¸°ì‚¬ì™€ ë²¡í„° ì¡°íšŒ
                await cursor.execute("""
                    SELECT 
                        a.article_id,
                        a.title,
                        a.summary,
                        a.published_at,
                        asv.vector,
                        asv.keywords
                    FROM articles a
                    INNER JOIN article_semantic_vectors asv 
                        ON a.article_id = asv.article_id
                    WHERE asv.vector IS NOT NULL
                """)

                all_articles = await cursor.fetchall()

                if not all_articles:
                    logger.warning("ë²¡í„°í™”ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return SearchResponse(query=query, totalCount=0, articles=[])

                # 3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ìƒˆë¡œìš´ ë¡œì§)
                search_results = []
                for article in all_articles:
                    try:
                        # DBì— ì €ì¥ëœ ë²¡í„°(í‚¤ì›Œë“œë³„ ë²¡í„° JSON)ì™€ í‚¤ì›Œë“œ(TF-IDF ì ìˆ˜ JSON) ë¡œë“œ
                        keyword_vectors = json.loads(article['vector'] or '{}')
                        tfidf_scores = json.loads(article['keywords'] or '{}')

                        if not keyword_vectors:
                            continue

                        # ê° í‚¤ì›Œë“œ ë²¡í„°ì™€ ê²€ìƒ‰ì–´ ë²¡í„°ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                        similarities = [
                            cosine_similarity(query_vector, np.array(vec))
                            for vec in keyword_vectors.values()
                        ]

                        # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì´ ê¸°ì‚¬ì˜ ìµœì¢… ì ìˆ˜ë¡œ ê²°ì •
                        max_similarity = max(similarities) if similarities else 0.0

                        if max_similarity >= threshold:
                            # í‚¤ì›Œë“œ DTO ìƒì„± (ì´ì œ tfidf_scores ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©)
                            keywords_dto = [
                                KeywordScore(word=w, tfidf=s)
                                for w, s in tfidf_scores.items()
                            ]

                            search_results.append({
                                'article_id': article['article_id'],
                                'title': article['title'],
                                'summary': article['summary'],
                                'score': float(max_similarity),
                                'keywords': keywords_dto,
                                'publishedAt': article['published_at'].isoformat() if article['published_at'] else None
                            })
                    except Exception as e:
                        logger.warning(f"ê¸°ì‚¬ {article['article_id']} ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue


                # 4. ìœ ì‚¬ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
                search_results.sort(key=lambda x: x['score'], reverse=True)

                # 5. í˜ì´ì§• ì²˜ë¦¬
                total_count = len(search_results)
                start_idx = page * size
                end_idx = start_idx + size
                paginated_results = search_results[start_idx:end_idx]

                # 6. ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                articles = []
                for result in paginated_results:
                    articles.append(ArticleSearchResult(
                        article_id=result['article_id'],
                        title=result['title'],
                        summary=result['summary'],
                        score=result['score'],
                        keywords=result['keywords'],
                        publishedAt=result['publishedAt']
                    ))

                logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ - ì „ì²´: {total_count}ê°œ, ë°˜í™˜: {len(articles)}ê°œ")

                return SearchResponse(
                    query=query,
                    totalCount=total_count,
                    articles=articles
                )

    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

async def vectorize_raw_query(query: str) -> Optional[np.ndarray]:
    """ê²€ìƒ‰ì–´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ë³€í™˜"""
    if not nlp_processor:
        logger.error("NLP í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None

    vector_dict = await nlp_processor.generate_sbert_vectors([query])
    if not vector_dict or query not in vector_dict:
        logger.error(f"ê²€ìƒ‰ì–´ '{query}'ì— ëŒ€í•œ SBERT ë²¡í„° ìƒì„± ì‹¤íŒ¨")
        return None

    return np.array(vector_dict[query])

def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        # ë²¡í„°ê°€ ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ìˆë‹¤ë©´ ë‹¨ìˆœ ë‚´ì ë§Œ ê³„ì‚°
        dot_product = np.dot(vec1, vec2)

        # ì•ˆì „ì„ ìœ„í•´ norm ì²´í¬
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        similarity = dot_product / (norm1 * norm2)

        # ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ë¡œ ì¸í•œ ë²”ìœ„ ë²—ì–´ë‚¨ ë°©ì§€
        return max(-1.0, min(1.0, similarity))

    except Exception as e:
        logger.error(f"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return 0.0

def parse_keywords(keywords_str: str) -> List[KeywordScore]:
    """í‚¤ì›Œë“œ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ KeywordScore ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    if not keywords_str:
        return []

    keywords = []
    try:
        # "word1:0.5,word2:0.3" í˜•ì‹ íŒŒì‹±
        pairs = keywords_str.split(',')
        for pair in pairs[:5]:  # ìƒìœ„ 5ê°œë§Œ
            if ':' in pair:
                word, score = pair.split(':', 1)
                keywords.append(KeywordScore(
                    word=word.strip(),
                    tfidf=float(score)
                ))
    except Exception as e:
        logger.warning(f"í‚¤ì›Œë“œ íŒŒì‹± ì˜¤ë¥˜: {e}")

    return keywords

# í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/test/search/{query}")
async def test_search(query: str):
    """ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    try:
        result = await search_articles_semantic(query, page=0, size=5)
        return {
            "status": "success",
            "query": query,
            "found": result.totalCount,
            "top_results": [
                {
                    "id": article.article_id,
                    "title": article.title[:50],
                    "score": article.score
                }
                for article in result.articles
            ]
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

ACTION_WEIGHTS = {
    "click": 1.0,
    "scrap": 1.5,  # ìŠ¤í¬ë©ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
}

async def _get_representative_vectors(article_ids: List[int]) -> Dict[int, np.ndarray]:
    """ ì—¬ëŸ¬ ê¸°ì‚¬ì˜ ëŒ€í‘œ ë²¡í„°ë¥¼ í•œë²ˆì— ì¡°íšŒ """
    if not article_ids or not db_pool:
        return {}

    vectors = {}
    query = f"SELECT article_id, representative_vector FROM article_semantic_vectors WHERE article_id IN ({','.join(['%s'] * len(article_ids))})"

    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            await cursor.execute(query, tuple(article_ids))
            rows = await cursor.fetchall()
            for row in rows:
                if row['representative_vector']:
                    vectors[row['article_id']] = np.array(json.loads(row['representative_vector']))
    return vectors

async def _calculate_user_profile_vector(interactions: List[UserInteraction]) -> Optional[np.ndarray]:
    """ í–‰ë™ ë¡œê·¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„° ê³„ì‚° """
    article_ids = [interaction.articleId for interaction in interactions]
    article_vectors = await _get_representative_vectors(article_ids)

    if not article_vectors:
        return None

    weighted_vectors = []
    total_weight = 0

    for interaction in interactions:
        article_id = interaction.articleId
        action = interaction.action

        if article_id in article_vectors:
            vector = article_vectors[article_id]
            weight = ACTION_WEIGHTS.get(action, 0.0)

            weighted_vectors.append(vector * weight)
            total_weight += weight

    if not weighted_vectors:
        return None

    # ê°€ì¤‘ í‰ê·  ê³„ì‚°
    avg_vector = np.sum(weighted_vectors, axis=0) / total_weight
    norm = np.linalg.norm(avg_vector)
    if norm > 0:
        return avg_vector / norm
    return avg_vector


# --- ë§ì¶¤ í”¼ë“œ ì—”ë“œí¬ì¸íŠ¸ ---

@app.post("/api/nlp/user-profile", status_code=status.HTTP_201_CREATED)
async def update_user_profile(request: UserProfileRequest):
    """
    ì‚¬ìš©ìì˜ í–‰ë™ ë¡œê·¸ë¥¼ ë°›ì•„ í”„ë¡œí•„ ë²¡í„°ë¥¼ ìƒì„±/ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    (Spring ë°±ì—”ë“œì—ì„œ í˜¸ì¶œ)
    """
    user_id = request.userId
    user_profile_vector = await _calculate_user_profile_vector(request.interactions)

    if user_profile_vector is None:
        raise HTTPException(status_code=400, detail="ìœ íš¨í•œ í–‰ë™ ë¡œê·¸ê°€ ì—†ì–´ í”„ë¡œí•„ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    vector_str = json.dumps(user_profile_vector.tolist())

    async with db_pool.acquire() as conn:
        async with conn.cursor() as cursor:
            # UPSERT (INSERT ... ON DUPLICATE KEY UPDATE)
            await cursor.execute("""
                INSERT INTO user_vectors (user_id, vector, model_version)
                VALUES (%s, %s, %s)
                ON DUPLICATE KEY UPDATE
                    vector = VALUES(vector),
                    model_version = VALUES(model_version),
                    updated_at = NOW()
            """, (user_id, vector_str, "profile-v1"))

    return {"userId": user_id, "status": "profile_updated"}

@app.get("/api/nlp/feed/{user_id}", response_model=FeedResponse)
async def get_personalized_feed(user_id: int, page: int = 0, size: int = 20):
    """
    ì €ì¥ëœ ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ì¶¤ ê¸°ì‚¬ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
    """
    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            # 1. ì‚¬ìš©ì ë²¡í„° ì¡°íšŒ
            await cursor.execute("SELECT vector FROM user_vectors WHERE user_id = %s", (user_id,))
            user_row = await cursor.fetchone()
            if not user_row or not user_row['vector']:
                raise HTTPException(status_code=404, detail="ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œí•„ì„ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”.")
            user_vector = np.array(json.loads(user_row['vector']))

            # 2. ëª¨ë“  ê¸°ì‚¬ì˜ ëŒ€í‘œ ë²¡í„° ì¡°íšŒ
            await cursor.execute("SELECT article_id, representative_vector FROM article_semantic_vectors")
            all_articles = await cursor.fetchall()

            # 3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            recommendations = []
            for article in all_articles:
                if article['representative_vector']:
                    article_vector = np.array(json.loads(article['representative_vector']))
                    score = cosine_similarity(user_vector, article_vector)
                    recommendations.append(RecommendedArticle(articleId=article['article_id'], score=score))

            # 4. ìœ ì‚¬ë„ ìˆœ ì •ë ¬ ë° í˜ì´ì§•
            recommendations.sort(key=lambda x: x.score, reverse=True)
            start_idx = page * size
            end_idx = start_idx + size

            return FeedResponse(articles=recommendations[start_idx:end_idx])

# --- ë©”ì¸ ì‹¤í–‰ ---

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=5000, reload=True)