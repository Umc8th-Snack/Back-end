# main.py
from fastapi import FastAPI, HTTPException, status, Query
from pydantic import BaseModel
import uvicorn
import asyncio
import logging
from typing import List, Dict, Any, Optional
import aiomysql
import os
from datetime import datetime
from dotenv import load_dotenv
import numpy as np

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ìˆì—ˆìŒ!) ---
app = FastAPI(
    title="ê¸°ì‚¬ NLP ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤",
    description="ê¸°ì‚¬ ë¶„ì„ ë° ì¶”ì²œì„ ìœ„í•œ í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ (TF-IDF + SBERT)",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# DB ì—°ê²° ì„¤ì • (.envì—ì„œ ì½ê¸°)
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': int(os.getenv('DB_PORT', 3306)),
    'user': os.getenv('DB_USER', 'root'),
    'password': os.getenv('DB_PASSWORD', ''),
    'db': os.getenv('DB_NAME', 'snack_db'),
    'charset': os.getenv('DB_CHARSET', 'utf8mb4'),
    'autocommit': True
}

# DB ì—°ê²° í’€ (ì „ì—­ ë³€ìˆ˜)
db_pool = None

# NLP í”„ë¡œì„¸ì„œ import (ì—†ìœ¼ë©´ ì„ì‹œ ëª¨ë“ˆ)
try:
    import nlp_processor
except ImportError:
    logger.warning("nlp_processor ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê¸°ëŠ¥ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    nlp_processor = None

# --- Pydantic ëª¨ë¸ ì •ì˜ ---

class ArticleVectorizeRequestDto(BaseModel):
    articleId: int
    title: str
    summary: str

class ArticleVectorizeListRequestDto(BaseModel):
    articles: List[ArticleVectorizeRequestDto]

class ArticleKeywordDto(BaseModel):
    word: str
    tfidf: float

class ArticleVectorizeResponseDto(BaseModel):
    articleId: int
    vector: List[float]
    keywords: List[ArticleKeywordDto]

class ArticleVectorizeListResponseDto(BaseModel):
    results: List[ArticleVectorizeResponseDto]

class ArticleDto(BaseModel):
    articleId: int
    title: str
    summary: str

class SearchArticleResponseDto(BaseModel):
    articles: List[ArticleDto]

# --- ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---

@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì‹¤í–‰"""
    global db_pool
    logger.info("=" * 50)
    logger.info("NLP ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘...")
    logger.info(f"DB ì—°ê²° ì„¤ì •: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['db']}")

    try:
        # DB ì—°ê²° í’€ ìƒì„±
        db_pool = await aiomysql.create_pool(
            **DB_CONFIG,
            minsize=5,
            maxsize=20
        )
        logger.info("âœ… DB ì—°ê²° í’€ ìƒì„± ì„±ê³µ")

        # ì—°ê²° í…ŒìŠ¤íŠ¸
        async with db_pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute("SELECT 1")
                result = await cursor.fetchone()
                logger.info(f"âœ… DB ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ: {result}")

        # NLP ëª¨ë¸ ì´ˆê¸°í™”
        if nlp_processor:
            await nlp_processor.initialize_nlp_service()
            logger.info("âœ… NLP ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        else:
            logger.warning("âš ï¸ NLP ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        logger.info("=" * 50)
        logger.info("ğŸš€ ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")

    except Exception as e:
        logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.error("ì„œë²„ëŠ” ê³„ì† ì‹¤í–‰ë˜ì§€ë§Œ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

@app.on_event("shutdown")
async def shutdown_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ì‹¤í–‰"""
    global db_pool
    logger.info("ì„œë¹„ìŠ¤ ì¢…ë£Œ ì¤‘...")

    if db_pool:
        db_pool.close()
        await db_pool.wait_closed()
        logger.info("DB ì—°ê²° í’€ ì¢…ë£Œ ì™„ë£Œ")

# --- í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ---

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "NLP Microservice",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/health")
async def health_check():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    status = {
        "service": "healthy",
        "database": "unknown",
        "nlp_model": "unknown"
    }

    # DB ì—°ê²° í™•ì¸
    if db_pool:
        try:
            async with db_pool.acquire() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute("SELECT 1")
                    status["database"] = "connected"
        except:
            status["database"] = "disconnected"

    # NLP ëª¨ë¸ í™•ì¸
    if nlp_processor and hasattr(nlp_processor, 'is_service_ready'):
        status["nlp_model"] = "loaded" if nlp_processor.is_service_ready() else "not loaded"

    return status

# --- ë²¡í„°í™” ì—”ë“œí¬ì¸íŠ¸ ---
# main.pyì˜ ìˆ˜ì •ëœ ë²¡í„°í™” í•¨ìˆ˜ ë¶€ë¶„

@app.post("/api/nlp/vectorize/articles")
async def vectorize_articles_from_db(article_ids: List[int]):
    """
    ê¸°ì‚¬ ID ëª©ë¡ì„ ë°›ì•„ DBì—ì„œ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  ë²¡í„°í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.

    article_semantic_vectors í…Œì´ë¸” êµ¬ì¡°:
    - article_id (PK)
    - vector
    - keywords
    - model_version
    - created_at
    - updated_at
    """
    if not article_ids:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="article_idsëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤."
        )

    if not db_pool:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            processed_count = 0
            failed_ids = []

            for article_id in article_ids:
                try:
                    # 1. DBì—ì„œ ê¸°ì‚¬ ì •ë³´ ì¡°íšŒ
                    await cursor.execute("""
                        SELECT article_id, title, summary 
                        FROM articles 
                        WHERE article_id = %s
                    """, (article_id,))

                    article = await cursor.fetchone()

                    if not article:
                        logger.warning(f"ê¸°ì‚¬ ID {article_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        failed_ids.append(article_id)
                        continue

                    # í…ìŠ¤íŠ¸ ê²°í•©
                    text = f"{article['title']} {article['summary']}"

                    # 2. NLP ì²˜ë¦¬
                    if nlp_processor:
                        # TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ
                        tfidf_keywords = await nlp_processor.extract_tfidf_keywords(text, top_k=10)

                        # SBERT ë²¡í„° ìƒì„±
                        top_keywords = list(tfidf_keywords.keys())
                        sbert_vectors = await nlp_processor.generate_sbert_vectors(top_keywords)

                        # ê°€ì¤‘ í‰ê·  ë²¡í„° ê³„ì‚°
                        weighted_vector = await calculate_weighted_average(sbert_vectors, tfidf_keywords)
                    else:
                        # nlp_processorê°€ ì—†ìœ¼ë©´ ë”ë¯¸ ë°ì´í„°
                        tfidf_keywords = {"í…ŒìŠ¤íŠ¸": 1.0}
                        weighted_vector = [0.0] * 384

                    # 3. ë²¡í„°ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                    vector_str = ','.join(map(str, weighted_vector))
                    keywords_str = ','.join([f"{k}:{v:.4f}" for k, v in tfidf_keywords.items()])

                    # 4. ê¸°ì¡´ ë²¡í„° í™•ì¸ (article_idë¡œ ì§ì ‘ í™•ì¸)
                    await cursor.execute("""
                        SELECT article_id 
                        FROM article_semantic_vectors 
                        WHERE article_id = %s
                    """, (article_id,))

                    existing = await cursor.fetchone()

                    # 5. DBì— ì €ì¥ ë˜ëŠ” ì—…ë°ì´íŠ¸
                    if existing:
                        # ê¸°ì¡´ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸
                        await cursor.execute("""
                            UPDATE article_semantic_vectors 
                            SET vector = %s, 
                                keywords = %s,
                                model_version = %s,
                                updated_at = NOW()
                            WHERE article_id = %s
                        """, (
                            f"[{vector_str}]",
                            keywords_str,
                            "tfidf-sbert-v1",
                            article_id
                        ))
                        logger.info(f"ê¸°ì‚¬ {article_id} ë²¡í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                    else:
                        # ìƒˆ ë ˆì½”ë“œ ì‚½ì…
                        await cursor.execute("""
                            INSERT INTO article_semantic_vectors 
                            (article_id, vector, keywords, model_version, created_at, updated_at)
                            VALUES (%s, %s, %s, %s, NOW(), NOW())
                        """, (
                            article_id,
                            f"[{vector_str}]",
                            keywords_str,
                            "tfidf-sbert-v1"
                        ))
                        logger.info(f"ê¸°ì‚¬ {article_id} ë²¡í„° ì‹ ê·œ ì €ì¥ ì™„ë£Œ")

                    processed_count += 1

                except Exception as e:
                    logger.error(f"ê¸°ì‚¬ {article_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    failed_ids.append(article_id)
                    # ê°œë³„ íŠ¸ëœì­ì…˜ ì‹¤íŒ¨ ì‹œ ë¡¤ë°±í•˜ì§€ ì•Šê³  ê³„ì† ì§„í–‰
                    continue

            return {
                "status": "completed",
                "total_requested": len(article_ids),
                "processed": processed_count,
                "failed": failed_ids,
                "message": f"{processed_count}ê°œ ê¸°ì‚¬ ë²¡í„°í™” ì™„ë£Œ"
            }

@app.post("/api/nlp/vectorize/batch")
async def batch_vectorize_articles(
        limit: int = Query(100, description="í•œ ë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜"),
        force_update: bool = Query(False, description="ê¸°ì¡´ ë²¡í„° ì¬ìƒì„± ì—¬ë¶€")
):
    """
    ë²¡í„°í™”ë˜ì§€ ì•Šì€ ê¸°ì‚¬ë“¤ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """

    if not db_pool:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            # ë²¡í„°í™”ë˜ì§€ ì•Šì€ ê¸°ì‚¬ ì°¾ê¸°
            if force_update:
                # ê°•ì œ ì—…ë°ì´íŠ¸: ëª¨ë“  ê¸°ì‚¬ ëŒ€ìƒ
                query = """
                    SELECT article_id 
                    FROM articles 
                    ORDER BY created_at DESC 
                    LIMIT %s
                """
            else:
                # ë²¡í„°ê°€ ì—†ëŠ” ê¸°ì‚¬ë§Œ ì„ íƒ
                query = """
                    SELECT a.article_id 
                    FROM articles a
                    LEFT JOIN article_semantic_vectors asv 
                        ON a.article_id = asv.article_id
                    WHERE asv.article_id IS NULL
                    ORDER BY a.created_at DESC
                    LIMIT %s
                """

            await cursor.execute(query, (limit,))
            articles = await cursor.fetchall()

            if not articles:
                return {
                    "status": "no_articles",
                    "message": "ì²˜ë¦¬í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            article_ids = [a['article_id'] for a in articles]
            logger.info(f"ë²¡í„°í™”í•  ê¸°ì‚¬ {len(article_ids)}ê°œ ë°œê²¬: {article_ids[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸

            # ë²¡í„°í™” ìˆ˜í–‰
            return await vectorize_articles_from_db(article_ids)

# í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/db/check-schema")
async def check_db_schema():
    """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì™€ ë°ì´í„° ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""

    if not db_pool:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            result = {}

            try:
                # article_semantic_vectors í…Œì´ë¸” êµ¬ì¡° í™•ì¸
                await cursor.execute("""
                    SHOW COLUMNS FROM article_semantic_vectors
                """)
                result['article_semantic_vectors_columns'] = await cursor.fetchall()
            except Exception as e:
                result['article_semantic_vectors_error'] = str(e)

            try:
                # articles í…Œì´ë¸” êµ¬ì¡° í™•ì¸
                await cursor.execute("""
                    SHOW COLUMNS FROM articles
                """)
                result['articles_columns'] = await cursor.fetchall()
            except Exception as e:
                result['articles_error'] = str(e)

            try:
                # í…Œì´ë¸” ë°ì´í„° ê°œìˆ˜ í™•ì¸
                await cursor.execute("""
                    SELECT 
                        (SELECT COUNT(*) FROM articles) as article_count,
                        (SELECT COUNT(*) FROM article_semantic_vectors) as vector_count
                """)
                counts = await cursor.fetchone()
                result['data_counts'] = counts

                # ë²¡í„°í™”ë˜ì§€ ì•Šì€ ê¸°ì‚¬ ê°œìˆ˜
                await cursor.execute("""
                    SELECT COUNT(*) as unvectorized_count
                    FROM articles a
                    LEFT JOIN article_semantic_vectors asv 
                        ON a.article_id = asv.article_id
                    WHERE asv.article_id IS NULL
                """)
                unvectorized = await cursor.fetchone()
                result['unvectorized_articles'] = unvectorized['unvectorized_count']

            except Exception as e:
                result['count_error'] = str(e)

            return result

# í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸ - íŠ¹ì • ê¸°ì‚¬ì˜ ë²¡í„° í™•ì¸
@app.get("/api/vectors/{article_id}")
async def get_article_vector(article_id: int):
    """íŠ¹ì • ê¸°ì‚¬ì˜ ë²¡í„° ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""

    if not db_pool:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            # ê¸°ì‚¬ ì •ë³´ ì¡°íšŒ
            await cursor.execute("""
                SELECT a.article_id, a.title, a.summary,
                       asv.vector, asv.keywords, asv.model_version,
                       asv.created_at, asv.updated_at
                FROM articles a
                LEFT JOIN article_semantic_vectors asv 
                    ON a.article_id = asv.article_id
                WHERE a.article_id = %s
            """, (article_id,))

            result = await cursor.fetchone()

            if not result:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"ê¸°ì‚¬ ID {article_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )

            # ë²¡í„°ê°€ ìˆìœ¼ë©´ ê¸¸ì´ ê³„ì‚°
            if result['vector']:
                vector_str = result['vector'].strip('[]')
                vector_length = len(vector_str.split(',')) if vector_str else 0
                result['vector_length'] = vector_length
                # ë²¡í„° ë‚´ìš©ì€ ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                vector_preview = ','.join(vector_str.split(',')[:10])
                result['vector_preview'] = f"[{vector_preview},...]"
            else:
                result['vector_length'] = 0
                result['vector_preview'] = None

            return result

# --- ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸ ---

@app.get("/api/articles/search/{keyword}")
async def search_articles_direct(
        keyword: str,
        page: int = Query(0, ge=0),
        size: int = Query(10, ge=1, le=50),
        threshold: float = Query(0.3, description="ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’")
):
    """í‚¤ì›Œë“œë¥¼ ë²¡í„°í™”í•˜ê³  DBì˜ ë²¡í„°ë“¤ê³¼ ì§ì ‘ ë¹„êµí•˜ì—¬ ìœ ì‚¬í•œ ê¸°ì‚¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""

    if not keyword or not keyword.strip():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="keywordëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤."
        )

    # ê°„ë‹¨í•œ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” nlp_processor ì‚¬ìš©)
    return SearchArticleResponseDto(articles=[])

# --- í—¬í¼ í•¨ìˆ˜ ---

async def calculate_weighted_average(
        sbert_vectors: dict,
        tfidf_scores: dict
) -> List[float]:
    """TF-IDF ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•˜ì—¬ SBERT ë²¡í„°ë“¤ì˜ ê°€ì¤‘ í‰ê·  ê³„ì‚°"""

    if not sbert_vectors:
        return [0.0] * 384

    vectors = []
    weights = []

    for keyword in sbert_vectors:
        if keyword in tfidf_scores:
            vectors.append(sbert_vectors[keyword])
            weights.append(tfidf_scores[keyword])

    if not vectors:
        return [0.0] * 384

    # numpy ë°°ì—´ë¡œ ë³€í™˜
    vectors = np.array(vectors)
    weights = np.array(weights)

    # ê°€ì¤‘ì¹˜ ì •ê·œí™”
    weights = weights / weights.sum()

    # ê°€ì¤‘ í‰ê·  ê³„ì‚°
    weighted_avg = np.average(vectors, axis=0, weights=weights)

    # ë²¡í„° ì •ê·œí™”
    norm = np.linalg.norm(weighted_avg)
    if norm > 0:
        weighted_avg = weighted_avg / norm

    return weighted_avg.tolist()



# --- ë©”ì¸ ì‹¤í–‰ ---

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=5000, reload=True)