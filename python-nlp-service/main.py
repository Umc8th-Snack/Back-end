# main.py
from fastapi import FastAPI, HTTPException, status, Query
from pydantic import BaseModel
import uvicorn
import asyncio
import logging
from typing import List, Dict, Any, Optional
import aiomysql
import os
from datetime import datetime
from dotenv import load_dotenv
import numpy as np
import json

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ìˆì—ˆìŒ!) ---
app = FastAPI(
    title="ê¸°ì‚¬ NLP ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤",
    description="ê¸°ì‚¬ ë¶„ì„ ë° ì¶”ì²œì„ ìœ„í•œ í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ (TF-IDF + SBERT)",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# DB ì—°ê²° ì„¤ì • (.envì—ì„œ ì½ê¸°)
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': int(os.getenv('DB_PORT', 3306)),
    'user': os.getenv('DB_USER', 'root'),
    'password': os.getenv('DB_PASSWORD', ''),
    'db': os.getenv('DB_NAME', 'snack_db'),
    'charset': os.getenv('DB_CHARSET', 'utf8mb4'),
    'autocommit': True
}

# DB ì—°ê²° í’€ (ì „ì—­ ë³€ìˆ˜)
db_pool = None

# NLP í”„ë¡œì„¸ì„œ import (ì—†ìœ¼ë©´ ì„ì‹œ ëª¨ë“ˆ)
try:
    import nlp_processor
except ImportError:
    logger.warning("nlp_processor ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê¸°ëŠ¥ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    nlp_processor = None

# --- Pydantic ëª¨ë¸ ì •ì˜ ---

class ArticleVectorizeRequestDto(BaseModel):
    articleId: int
    title: str
    summary: str

class ArticleVectorizeListRequestDto(BaseModel):
    articles: List[ArticleVectorizeRequestDto]

class ArticleKeywordDto(BaseModel):
    word: str
    tfidf: float

class ArticleVectorizeResponseDto(BaseModel):
    articleId: int
    vector: List[float]
    keywords: List[ArticleKeywordDto]

class ArticleVectorizeListResponseDto(BaseModel):
    results: List[ArticleVectorizeResponseDto]

class ArticleDto(BaseModel):
    articleId: int
    title: str
    summary: str

class SearchArticleResponseDto(BaseModel):
    articles: List[ArticleDto]

# DTO ëª¨ë¸ ì •ì˜
class KeywordScore(BaseModel):
    word: str
    tfidf: float

class ArticleSearchResult(BaseModel):
    article_id: int
    title: str
    summary: Optional[str] = None
    score: float
    keywords: List[KeywordScore]
    publishedAt: Optional[str]

class SearchResponse(BaseModel):
    query: str
    totalCount: int
    articles: List[ArticleSearchResult]

# --- ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---

@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì‹¤í–‰"""
    global db_pool
    logger.info("=" * 50)
    logger.info("NLP ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘...")
    logger.info(f"DB ì—°ê²° ì„¤ì •: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['db']}")

    try:
        # DB ì—°ê²° í’€ ìƒì„±
        db_pool = await aiomysql.create_pool(
            **DB_CONFIG,
            minsize=5,
            maxsize=20
        )
        logger.info("âœ… DB ì—°ê²° í’€ ìƒì„± ì„±ê³µ")

        # ì—°ê²° í…ŒìŠ¤íŠ¸
        async with db_pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute("SELECT 1")
                result = await cursor.fetchone()
                logger.info(f"âœ… DB ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ: {result}")

        # NLP ëª¨ë¸ ì´ˆê¸°í™”
        if nlp_processor:
            await nlp_processor.initialize_nlp_service()
            logger.info("âœ… NLP ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        else:
            logger.warning("âš ï¸ NLP ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        logger.info("=" * 50)
        logger.info("ğŸš€ ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")

    except Exception as e:
        logger.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.error("ì„œë²„ëŠ” ê³„ì† ì‹¤í–‰ë˜ì§€ë§Œ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

@app.on_event("shutdown")
async def shutdown_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ì‹¤í–‰"""
    global db_pool
    logger.info("ì„œë¹„ìŠ¤ ì¢…ë£Œ ì¤‘...")

    if db_pool:
        db_pool.close()
        await db_pool.wait_closed()
        logger.info("DB ì—°ê²° í’€ ì¢…ë£Œ ì™„ë£Œ")

# --- í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ---

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "NLP Microservice",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/health")
async def health_check():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    status = {
        "service": "healthy",
        "database": "unknown",
        "nlp_model": "unknown"
    }

    # DB ì—°ê²° í™•ì¸
    if db_pool:
        try:
            async with db_pool.acquire() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute("SELECT 1")
                    status["database"] = "connected"
        except:
            status["database"] = "disconnected"

    # NLP ëª¨ë¸ í™•ì¸
    if nlp_processor and hasattr(nlp_processor, 'is_service_ready'):
        status["nlp_model"] = "loaded" if nlp_processor.is_service_ready() else "not loaded"

    return status

# --- ë²¡í„°í™” ì—”ë“œí¬ì¸íŠ¸ ---
# main.pyì˜ ìˆ˜ì •ëœ ë²¡í„°í™” í•¨ìˆ˜ ë¶€ë¶„

@app.post("/api/nlp/vectorize/articles")
async def vectorize_articles_from_db(article_ids: List[int]):
    if not article_ids:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="article_idsëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
    if not db_pool:
        raise HTTPException(status_code=status.HTTP_503_SERVICE_UNAVAILABLE, detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")

    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            processed_count, failed_ids = 0, []
            for article_id in article_ids:
                try:
                    await cursor.execute("SELECT article_id, title, summary FROM articles WHERE article_id = %s", (article_id,))
                    article = await cursor.fetchone()
                    if not article:
                        logger.warning(f"ê¸°ì‚¬ ID {article_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        failed_ids.append(article_id)
                        continue

                    text = article.get('summary') or ''

                    tfidf_keywords = {}
                    sbert_vectors = {}

                    # --- ğŸš¨ ì—¬ê¸°ê°€ í•µì‹¬ ìˆ˜ì • ì‚¬í•­ì…ë‹ˆë‹¤ ---
                    if not text.strip():
                        # summaryê°€ ë¹„ì–´ìˆìœ¼ë©´, ë²¡í„°ì™€ í‚¤ì›Œë“œë¥¼ ëª¨ë‘ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì²˜ë¦¬
                        logger.info(f"ê¸°ì‚¬ {article_id}ëŠ” summaryê°€ ì—†ì–´ ë¹ˆ ê°’ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                    elif nlp_processor:
                        # summaryê°€ ìˆì„ ë•Œë§Œ NLP ì²˜ë¦¬
                        tfidf_keywords = await nlp_processor.extract_tfidf_keywords(text, top_k=10)
                        top_keywords = list(tfidf_keywords.keys())
                        if top_keywords:
                            sbert_vectors = await nlp_processor.generate_sbert_vectors(top_keywords)

                    vector_json_str = json.dumps(sbert_vectors)
                    keywords_json_str = json.dumps(tfidf_keywords)

                    await cursor.execute("SELECT article_id FROM article_semantic_vectors WHERE article_id = %s", (article_id,))
                    existing = await cursor.fetchone()
                    if existing:
                        await cursor.execute(
                            "UPDATE article_semantic_vectors SET vector = %s, keywords = %s, model_version = %s, updated_at = NOW() WHERE article_id = %s",
                            (vector_json_str, keywords_json_str, "sbert-keywords-v3", article_id)
                        )
                        logger.info(f"ê¸°ì‚¬ {article_id} í‚¤ì›Œë“œ ë²¡í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                    else:
                        await cursor.execute(
                            "INSERT INTO article_semantic_vectors (article_id, vector, keywords, model_version, created_at, updated_at) VALUES (%s, %s, %s, %s, NOW(), NOW())",
                            (article_id, vector_json_str, keywords_json_str, "sbert-keywords-v3")
                        )
                        logger.info(f"ê¸°ì‚¬ {article_id} í‚¤ì›Œë“œ ë²¡í„° ì‹ ê·œ ì €ì¥ ì™„ë£Œ")
                    processed_count += 1
                except Exception as e:
                    logger.error(f"ê¸°ì‚¬ {article_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                    failed_ids.append(article_id)
                    continue
            return {"status": "completed", "total_requested": len(article_ids), "processed": processed_count, "failed": failed_ids}

@app.post("/api/nlp/vectorize/batch")
async def batch_vectorize_articles(
        limit: int = Query(100, description="í•œ ë²ˆì— ì²˜ë¦¬í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜"),
        force_update: bool = Query(False, description="ê¸°ì¡´ ë²¡í„° ì¬ìƒì„± ì—¬ë¶€")
):
    """
    ë²¡í„°í™”ë˜ì§€ ì•Šì€ ê¸°ì‚¬ë“¤ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """

    if not db_pool:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            # ë²¡í„°í™”ë˜ì§€ ì•Šì€ ê¸°ì‚¬ ì°¾ê¸°
            if force_update:
                # ê°•ì œ ì—…ë°ì´íŠ¸: ëª¨ë“  ê¸°ì‚¬ ëŒ€ìƒ
                query = """
                    SELECT article_id 
                    FROM articles 
                    ORDER BY created_at DESC 
                    LIMIT %s
                """
            else:
                # ë²¡í„°ê°€ ì—†ëŠ” ê¸°ì‚¬ë§Œ ì„ íƒ
                query = """
                    SELECT a.article_id 
                    FROM articles a
                    LEFT JOIN article_semantic_vectors asv 
                        ON a.article_id = asv.article_id
                    WHERE asv.article_id IS NULL
                    ORDER BY a.created_at DESC
                    LIMIT %s
                """

            await cursor.execute(query, (limit,))
            articles = await cursor.fetchall()

            if not articles:
                return {
                    "status": "no_articles",
                    "message": "ì²˜ë¦¬í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤."
                }

            article_ids = [a['article_id'] for a in articles]
            logger.info(f"ë²¡í„°í™”í•  ê¸°ì‚¬ {len(article_ids)}ê°œ ë°œê²¬: {article_ids[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ ë¡œê·¸

            # ë²¡í„°í™” ìˆ˜í–‰
            return await vectorize_articles_from_db(article_ids)

# í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/db/check-schema")
async def check_db_schema():
    """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆì™€ ë°ì´í„° ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""

    if not db_pool:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            result = {}

            try:
                # article_semantic_vectors í…Œì´ë¸” êµ¬ì¡° í™•ì¸
                await cursor.execute("""
                    SHOW COLUMNS FROM article_semantic_vectors
                """)
                result['article_semantic_vectors_columns'] = await cursor.fetchall()
            except Exception as e:
                result['article_semantic_vectors_error'] = str(e)

            try:
                # articles í…Œì´ë¸” êµ¬ì¡° í™•ì¸
                await cursor.execute("""
                    SHOW COLUMNS FROM articles
                """)
                result['articles_columns'] = await cursor.fetchall()
            except Exception as e:
                result['articles_error'] = str(e)

            try:
                # í…Œì´ë¸” ë°ì´í„° ê°œìˆ˜ í™•ì¸
                await cursor.execute("""
                    SELECT 
                        (SELECT COUNT(*) FROM articles) as article_count,
                        (SELECT COUNT(*) FROM article_semantic_vectors) as vector_count
                """)
                counts = await cursor.fetchone()
                result['data_counts'] = counts

                # ë²¡í„°í™”ë˜ì§€ ì•Šì€ ê¸°ì‚¬ ê°œìˆ˜
                await cursor.execute("""
                    SELECT COUNT(*) as unvectorized_count
                    FROM articles a
                    LEFT JOIN article_semantic_vectors asv 
                        ON a.article_id = asv.article_id
                    WHERE asv.article_id IS NULL
                """)
                unvectorized = await cursor.fetchone()
                result['unvectorized_articles'] = unvectorized['unvectorized_count']

            except Exception as e:
                result['count_error'] = str(e)

            return result

# í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸ - íŠ¹ì • ê¸°ì‚¬ì˜ ë²¡í„° í™•ì¸
@app.get("/api/vectors/{article_id}")
async def get_article_vector(article_id: int):
    """íŠ¹ì • ê¸°ì‚¬ì˜ ë²¡í„° ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""

    if not db_pool:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    async with db_pool.acquire() as conn:
        async with conn.cursor(aiomysql.DictCursor) as cursor:
            # ê¸°ì‚¬ ì •ë³´ ì¡°íšŒ
            await cursor.execute("""
                SELECT a.article_id, a.title, a.summary,
                       asv.vector, asv.keywords, asv.model_version,
                       asv.created_at, asv.updated_at
                FROM articles a
                LEFT JOIN article_semantic_vectors asv 
                    ON a.article_id = asv.article_id
                WHERE a.article_id = %s
            """, (article_id,))

            result = await cursor.fetchone()

            if not result:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"ê¸°ì‚¬ ID {article_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )

            # ë²¡í„°ê°€ ìˆìœ¼ë©´ ê¸¸ì´ ê³„ì‚°
            if result['vector']:
                vector_str = result['vector'].strip('[]')
                vector_length = len(vector_str.split(',')) if vector_str else 0
                result['vector_length'] = vector_length
                # ë²¡í„° ë‚´ìš©ì€ ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                vector_preview = ','.join(vector_str.split(',')[:10])
                result['vector_preview'] = f"[{vector_preview},...]"
            else:
                result['vector_length'] = 0
                result['vector_preview'] = None

            return result

# --- ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸ ---

@app.get("/api/articles/search/{keyword}")
async def search_articles_direct(
        keyword: str,
        page: int = Query(0, ge=0),
        size: int = Query(10, ge=1, le=50),
        threshold: float = Query(0.3, description="ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’")
):
    """í‚¤ì›Œë“œë¥¼ ë²¡í„°í™”í•˜ê³  DBì˜ ë²¡í„°ë“¤ê³¼ ì§ì ‘ ë¹„êµí•˜ì—¬ ìœ ì‚¬í•œ ê¸°ì‚¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""

    if not keyword or not keyword.strip():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="keywordëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤."
        )

    # ê°„ë‹¨í•œ êµ¬í˜„ (ì‹¤ì œë¡œëŠ” nlp_processor ì‚¬ìš©)
    return SearchArticleResponseDto(articles=[])

# --- í—¬í¼ í•¨ìˆ˜ ---

async def calculate_weighted_average(
        sbert_vectors: dict,
        tfidf_scores: dict
) -> List[float]:
    """TF-IDF ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•˜ì—¬ SBERT ë²¡í„°ë“¤ì˜ ê°€ì¤‘ í‰ê·  ê³„ì‚°"""

    if not sbert_vectors:
        return [0.0] * 384

    vectors = []
    weights = []

    for keyword in sbert_vectors:
        if keyword in tfidf_scores:
            vectors.append(sbert_vectors[keyword])
            weights.append(tfidf_scores[keyword])

    if not vectors:
        return [0.0] * 384

    # numpy ë°°ì—´ë¡œ ë³€í™˜
    vectors = np.array(vectors)
    weights = np.array(weights)

    # ê°€ì¤‘ì¹˜ ì •ê·œí™”
    weights = weights / weights.sum()

    # ê°€ì¤‘ í‰ê·  ê³„ì‚°
    weighted_avg = np.average(vectors, axis=0, weights=weights)

    # ë²¡í„° ì •ê·œí™”
    norm = np.linalg.norm(weighted_avg)
    if norm > 0:
        weighted_avg = weighted_avg / norm

    return weighted_avg.tolist()

@app.post("/api/nlp/search")
async def search_articles_semantic(
        query: str,
        page: int = Query(0, ge=0, description="í˜ì´ì§€ ë²ˆí˜¸"),
        size: int = Query(10, ge=1, le=50, description="í˜ì´ì§€ í¬ê¸°"),
        threshold: float = Query(0.3, ge=0, le=1, description="ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’")
):
    """
    ì˜ë¯¸ ê¸°ë°˜ ê¸°ì‚¬ ê²€ìƒ‰
    1. ê²€ìƒ‰ì–´ë¥¼ TF-IDFë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
    2. í‚¤ì›Œë“œë“¤ì˜ SBERT ë²¡í„° ìƒì„±
    3. TF-IDF ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì¿¼ë¦¬ ë²¡í„° ìƒì„±
    4. DBì˜ ëª¨ë“  ê¸°ì‚¬ ë²¡í„°ì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    5. ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
    """

    if not query or not query.strip():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
        )

    if not db_pool:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤."
        )

    logger.info(f"ğŸ” ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹œì‘ - ê²€ìƒ‰ì–´: '{query}', í˜ì´ì§€: {page}, í¬ê¸°: {size}")

    try:
        # 1. ê²€ìƒ‰ì–´ ë²¡í„°í™”
        query_vector = await vectorize_raw_query(query)

        if query_vector is None:
            logger.error("ê²€ìƒ‰ì–´ ë²¡í„°í™” ì‹¤íŒ¨")
            return SearchResponse(query=query, totalCount=0, articles=[])

        # 2. DBì—ì„œ ëª¨ë“  ê¸°ì‚¬ì™€ ë²¡í„° ì¡°íšŒ
        async with db_pool.acquire() as conn:
            async with conn.cursor(aiomysql.DictCursor) as cursor:
                # ëª¨ë“  ê¸°ì‚¬ì™€ ë²¡í„° ì¡°íšŒ
                await cursor.execute("""
                    SELECT 
                        a.article_id,
                        a.title,
                        a.summary,
                        a.published_at,
                        asv.vector,
                        asv.keywords
                    FROM articles a
                    INNER JOIN article_semantic_vectors asv 
                        ON a.article_id = asv.article_id
                    WHERE asv.vector IS NOT NULL
                """)

                all_articles = await cursor.fetchall()

                if not all_articles:
                    logger.warning("ë²¡í„°í™”ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return SearchResponse(query=query, totalCount=0, articles=[])

                # 3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ìƒˆë¡œìš´ ë¡œì§)
                search_results = []
                for article in all_articles:
                    try:
                        # DBì— ì €ì¥ëœ ë²¡í„°(í‚¤ì›Œë“œë³„ ë²¡í„° JSON)ì™€ í‚¤ì›Œë“œ(TF-IDF ì ìˆ˜ JSON) ë¡œë“œ
                        keyword_vectors = json.loads(article['vector'] or '{}')
                        tfidf_scores = json.loads(article['keywords'] or '{}')

                        if not keyword_vectors:
                            continue

                        # ê° í‚¤ì›Œë“œ ë²¡í„°ì™€ ê²€ìƒ‰ì–´ ë²¡í„°ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                        similarities = [
                            cosine_similarity(query_vector, np.array(vec))
                            for vec in keyword_vectors.values()
                        ]

                        # ê°€ì¥ ë†’ì€ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì´ ê¸°ì‚¬ì˜ ìµœì¢… ì ìˆ˜ë¡œ ê²°ì •
                        max_similarity = max(similarities) if similarities else 0.0

                        if max_similarity >= threshold:
                            # í‚¤ì›Œë“œ DTO ìƒì„± (ì´ì œ tfidf_scores ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©)
                            keywords_dto = [
                                KeywordScore(word=w, tfidf=s)
                                for w, s in tfidf_scores.items()
                            ]

                            search_results.append({
                                'article_id': article['article_id'],
                                'title': article['title'],
                                'summary': article['summary'],
                                'score': float(max_similarity),
                                'keywords': keywords_dto,
                                'publishedAt': article['published_at'].isoformat() if article['published_at'] else None
                            })
                    except Exception as e:
                        logger.warning(f"ê¸°ì‚¬ {article['article_id']} ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue


                # 4. ìœ ì‚¬ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
                search_results.sort(key=lambda x: x['score'], reverse=True)

                # 5. í˜ì´ì§• ì²˜ë¦¬
                total_count = len(search_results)
                start_idx = page * size
                end_idx = start_idx + size
                paginated_results = search_results[start_idx:end_idx]

                # 6. ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                articles = []
                for result in paginated_results:
                    articles.append(ArticleSearchResult(
                        article_id=result['article_id'],
                        title=result['title'],
                        summary=result['summary'],
                        score=result['score'],
                        keywords=result['keywords'],
                        publishedAt=result['publishedAt']
                    ))

                logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ - ì „ì²´: {total_count}ê°œ, ë°˜í™˜: {len(articles)}ê°œ")

                return SearchResponse(
                    query=query,
                    totalCount=total_count,
                    articles=articles
                )

    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )
'''
async def vectorize_query(query: str) -> Optional[np.ndarray]:
    """
    ê²€ìƒ‰ì–´ë¥¼ ì˜ë¯¸ ë²¡í„°ë¡œ ë³€í™˜
    1. TF-IDFë¡œ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
    2. í‚¤ì›Œë“œë“¤ì˜ SBERT ë²¡í„° ìƒì„±
    3. TF-IDF ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•œ ê°€ì¤‘ í‰ê·  ê³„ì‚°
    """
    try:
        if not nlp_processor:
            logger.error("NLP í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        # 1. TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ (ìƒìœ„ 5ê°œ)
        tfidf_keywords = await nlp_processor.extract_tfidf_keywords(query, top_k=5)

        if not tfidf_keywords:
            logger.warning(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {query}")
            # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì „ì²´ ì¿¼ë¦¬ë¥¼ í•˜ë‚˜ì˜ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
            tfidf_keywords = {query: 1.0}

        # 2. í‚¤ì›Œë“œë“¤ì˜ SBERT ë²¡í„° ìƒì„±
        keywords_list = list(tfidf_keywords.keys())
        sbert_vectors = await nlp_processor.generate_sbert_vectors(keywords_list)

        if not sbert_vectors:
            logger.error("SBERT ë²¡í„° ìƒì„± ì‹¤íŒ¨")
            return None

        # 3. TF-IDF ê°€ì¤‘ í‰ê·  ê³„ì‚°
        vectors = []
        weights = []

        for keyword in keywords_list:
            if keyword in sbert_vectors and keyword in tfidf_keywords:
                vectors.append(sbert_vectors[keyword])
                weights.append(tfidf_keywords[keyword])

        if not vectors:
            logger.error("ë²¡í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None

        # numpy ë°°ì—´ë¡œ ë³€í™˜
        vectors = np.array(vectors)
        weights = np.array(weights)

        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        weights = weights / weights.sum()

        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_avg = np.average(vectors, axis=0, weights=weights)

        # ë²¡í„° ì •ê·œí™” (ë‹¨ìœ„ ë²¡í„°ë¡œ)
        norm = np.linalg.norm(weighted_avg)
        if norm > 0:
            weighted_avg = weighted_avg / norm

        logger.info(f"ì¿¼ë¦¬ ë²¡í„° ìƒì„± ì™„ë£Œ - í‚¤ì›Œë“œ: {keywords_list}")
        return weighted_avg

    except Exception as e:
        logger.error(f"ì¿¼ë¦¬ ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {e}")
        return None '''

async def vectorize_raw_query(query: str) -> Optional[np.ndarray]:
    """ê²€ìƒ‰ì–´ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ë³€í™˜"""
    if not nlp_processor:
        logger.error("NLP í”„ë¡œì„¸ì„œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None

    vector_dict = await nlp_processor.generate_sbert_vectors([query])
    if not vector_dict or query not in vector_dict:
        logger.error(f"ê²€ìƒ‰ì–´ '{query}'ì— ëŒ€í•œ SBERT ë²¡í„° ìƒì„± ì‹¤íŒ¨")
        return None

    return np.array(vector_dict[query])

def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        # ë²¡í„°ê°€ ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ìˆë‹¤ë©´ ë‹¨ìˆœ ë‚´ì ë§Œ ê³„ì‚°
        dot_product = np.dot(vec1, vec2)

        # ì•ˆì „ì„ ìœ„í•´ norm ì²´í¬
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        similarity = dot_product / (norm1 * norm2)

        # ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ë¡œ ì¸í•œ ë²”ìœ„ ë²—ì–´ë‚¨ ë°©ì§€
        return max(-1.0, min(1.0, similarity))

    except Exception as e:
        logger.error(f"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return 0.0

def parse_keywords(keywords_str: str) -> List[KeywordScore]:
    """í‚¤ì›Œë“œ ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ KeywordScore ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    if not keywords_str:
        return []

    keywords = []
    try:
        # "word1:0.5,word2:0.3" í˜•ì‹ íŒŒì‹±
        pairs = keywords_str.split(',')
        for pair in pairs[:5]:  # ìƒìœ„ 5ê°œë§Œ
            if ':' in pair:
                word, score = pair.split(':', 1)
                keywords.append(KeywordScore(
                    word=word.strip(),
                    tfidf=float(score)
                ))
    except Exception as e:
        logger.warning(f"í‚¤ì›Œë“œ íŒŒì‹± ì˜¤ë¥˜: {e}")

    return keywords

# í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/test/search/{query}")
async def test_search(query: str):
    """ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    try:
        result = await search_articles_semantic(query, page=0, size=5)
        return {
            "status": "success",
            "query": query,
            "found": result.totalCount,
            "top_results": [
                {
                    "id": article.article_id,
                    "title": article.title[:50],
                    "score": article.score
                }
                for article in result.articles
            ]
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

# --- ë©”ì¸ ì‹¤í–‰ ---

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=5000, reload=True)