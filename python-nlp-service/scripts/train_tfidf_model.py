# scripts/train_tfidf_model.py
import pickle
import re
import pymysql
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
import os
from dotenv import load_dotenv
from scripts.tokenizer_utils import dummy_tokenizer


# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œ
load_dotenv(os.path.join(os.path.dirname(__file__), '..', '.env'))

# --- ì„¤ì • ---
TFIDF_MODEL_PATH = os.path.join(os.path.dirname(__file__), '..', 'data', 'tfidf_vectorizer.pkl')

DB_HOST = os.getenv('DB_HOST', 'localhost')
DB_PORT = int(os.getenv('DB_PORT', 3306))
DB_USER = os.getenv('DB_USER', 'root')
DB_PASSWORD = os.getenv('DB_PASSWORD', 'your_password')
DB_NAME = os.getenv('DB_NAME', 'your_database')

# --- í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ---
okt = Okt()

# --- ë¶ˆìš©ì–´ ë¡œë”© ---
stopwords_set = set()
try:
    conn = pymysql.connect(
        host=DB_HOST, port=DB_PORT, user=DB_USER, password=DB_PASSWORD, db=DB_NAME, charset='utf8mb4'
    )
    cursor = conn.cursor()
    cursor.execute("SELECT word FROM stopwords")
    stopwords_set = {row[0] for row in cursor.fetchall()}
    cursor.close()
    conn.close()
    print(f"í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë¶ˆìš©ì–´ ë¡œë“œ ì™„ë£Œ. ê°œìˆ˜: {len(stopwords_set)}")
except pymysql.Error as e:
    print(f"ê²½ê³ : ë¶ˆìš©ì–´ DB ë¡œë“œ ì‹¤íŒ¨: {e}. ë¶ˆìš©ì–´ ì œê±° ì—†ì´ ì§„í–‰.")
    stopwords_set = set()

# --- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í¬ë‚˜ì´ì§• ---
def tokenize_korean_text_for_tfidf(text: str) -> list[str]:
    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    tokens = okt.pos(text, norm=True, stem=True)
    return [
        word for word, pos in tokens
        if pos.startswith('N') and word not in stopwords_set and len(word) > 1
    ]



# --- ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ ---
def train_and_save_tfidf_model(article_contents: list[str]):
    if not article_contents:
        print("TF-IDF í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì—†ìŒ. í•™ìŠµ ìŠ¤í‚µ.")
        return

    print("TF-IDF ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    tfidf_vectorizer = TfidfVectorizer(
        tokenizer=dummy_tokenizer,
        min_df=2, #ì¼ë‹¨ ë‘ ê°œë¡œ..
        max_df=0.8
    )
    tfidf_vectorizer.fit(article_contents)
    print(f"í•™ìŠµ ì™„ë£Œ. ì–´íœ˜ ìˆ˜: {len(tfidf_vectorizer.vocabulary_)}")

    os.makedirs(os.path.dirname(TFIDF_MODEL_PATH), exist_ok=True)
    with open(TFIDF_MODEL_PATH, 'wb') as f:
        pickle.dump(tfidf_vectorizer, f)
    print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {TFIDF_MODEL_PATH}")

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    print("--- TF-IDF ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ---")

    article_contents_from_db = []
    try:
        conn = pymysql.connect(
            host=DB_HOST, port=DB_PORT, user=DB_USER, password=DB_PASSWORD, db=DB_NAME, charset='utf8mb4'
        )
        cursor = conn.cursor()
        cursor.execute("SELECT summary FROM articles")
        rows = cursor.fetchall()
        # ğŸ”¥ None ì œê±° + ê³µë°± ì œê±°
        article_contents_from_db = [
            row[0].strip() for row in rows if row[0] and isinstance(row[0], str) and row[0].strip()
        ]
        cursor.close()
        conn.close()
        print(f"DBì—ì„œ {len(article_contents_from_db)}ê°œì˜ summary ë¡œë“œ ì™„ë£Œ.")
    except pymysql.Error as e:
        print(f"DB ì—ëŸ¬: {e}")
        exit(1)
    except Exception as e:
        print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        exit(1)

    train_and_save_tfidf_model(article_contents_from_db)

    print("--- TF-IDF ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ ---")
